#+TITLE: Automatic Differentiation Manifesto
#+AUTHOR: Richard Wei
#+TIME: <2017-03-10 Fri> 

* Introduction

We discuss a standard intermediate representation for DLVM to match the standard
algorithm of automatic differentiation. And we aim, in DLVM, to achieve more
generic function representation compared to those implemented in deep learning
software such as TensorFlow.

* Problem

Automatic differentiation, including forward mode and backward mode, can be
implemented in either source code transformation or augmented data structures

Originally we added an DAG abstraction called ~Section~ to enable shared common
derivative \( \frac{\partial f}{\partial g} \) for the following case of
differentiation:
\begin{align*}
f(x, y) &= (g(x, y))^2 \\
\dfrac{\partial f}{\partial x}
    &= \dfrac{\partial f}{\partial g} \dfrac{\partial g}{\partial x} 
     = 2g(x, y) \dfrac{\partial g}{\partial x}
\end{align*}


This is then a very close representation to the augmented data structure
approach, with the benefits of partial computation--if we need the value of
forward propagation \(f(x, y)\), we can compute the "top" section after
computing all of its dependencies; if we need \(\frac{\partial f}{\partial x}\),
we can compute the section tagged "~deriving x~" without computing other leaf
nodes, by calling the ~diff~ instruction on the output of forward computation.
The underlying implementation of diff would be implicitly taking a partially
evaluated graph and continue to compute the dependencies of \(\frac{\partial
f}{\partial x}\).

However, this design has many problems:

1) Two data flow graph represnetations now emerge--the def-use graph and the
   section graph. This additional layer of complexity would make data flow
   analysis unnecessarily complicated.
2) Implicit return of partially evaluated graphs. This requires implicitly
   returning a struct, remembering all the defs of the forward pass. It is
   unclear how we deal with control flow with such implicit structs.
   - The code sharing between the forward pass and the backward pass intended to
     achieve the same behavior as the augmented graph paradigm, a.k.a. the data
     flow graph in TensorFlow, Marian, etc. But, the output values of nodes are
     not always used during derivative computation. In fact, such cases, like
     \(\frac{d\sigma(x)}{dx} = \sigma(x)(1-\sigma(x))\) and \(\frac{de^x}{dx} =
     e^x\), are very rare. Moreover, users usually don't want the output of the
     original function when they simply need the gradient to learn the weights.
3) No support for higher-order derivatives. If we were to support
   \(\frac{\partial^2 f}{\partial x^2}\) in this paradigm, do we append the
   computation to the original graph with special tags? This seems extremely
   ad-hoc.

* Solution

** Remove ~Section~

   In [[http://www.sciencedirect.com/science/article/pii/S0377042700004222][Automatic differentiation of algorithms]], the source code transformation of
   automatic differentiation is to define a new function that computes the
   partial derivatives with respect to every argument of the original function. The
   new function returns multiple values as a tuple. Formally, a function of type
   \begin{gather}
   (\mathbb{R}_1, \mathbb{R}_2, ..., \mathbb{R}_n) \rightarrow \mathbb{R}
   \end{gather}
   will be transformed to
   \begin{gather}
   (\mathbb{R}_1, \mathbb{R}_2, ..., \mathbb{R}_n) \rightarrow 
        (\mathbb{R}_1, \mathbb{R}_2, ..., \mathbb{R}_n)
   \end{gather}

** Allow multiple exits

   The standard algorithm of automatic differentiation (using /Wengert lists/)
   can operate on the control flow graph directly without having to worry about
   exits. We will allow functions to have multiply exits.

** Purify DLVM functions

   Make pure functions, functions that do not mutate a global variable,
   differentiable. Make all other functions non-differentiable.

* Detailed Design

** SSA form

   In [[http://www.sciencedirect.com/science/article/pii/S0377042700004222][Automatic Differentiation of Algorithms]], code in a function is represented as
   a /Wengert list/:
   #+BEGIN_QUOTE
   In order to describe the reverse accumulation technique, we need to untangle the
   relationship between a mathematical variable and a program variable. In this
   section we describe for this purpose an abstraction called a Wengert list [24].
   We can think of a Wengert list as a trace of a particular run of a program, with
   specific values for the inputs. The only statements which occur in the Wengert
   list are assignment statements to nonoverwritable variables called Wengert
   variables. The Wengert list abstracts away from all control-flow considerations:
   all loops are unrolled, all procedure calls are inlined, and all conditional
   statements are replaced by the taken branch. Consequently, the Wengert list may
   be many times longer than the text of the program to which it corresponds.
   #+END_QUOTE

   This is in fact SSA form, which we have already built.

   As a side note, in order to reduce special cases and minimize the effort
   spent on the infrastructure, we decide to adopt SIL (Swift Intermediate
   Language)'s argument-consuming basic blocks. It's a more uniform IR where the
   entry block is no longer a special case, but simply taking the same arguments
   that the parent function takes. Phi instructions is no longer needed, since
   argument-taking basic blocks function like closures in CPS (continuation
   passing style). More benefits of this kind of IR is described in [[http://compilers.cs.uni-saarland.de/papers/lkh15_cgo.pdf][A
   Graph-Based Higher-Order Intermediate Representation]], which will be discussed
   in the LLVM group's reading group later this semester.

** Type system

   Refactor the IR "type system" to support \(n\)-ary tuples such as ~([1x2] f32, [2x2]
   f32)~, and allow functions to return a tuple.

** Instructions

*** Remove ~diff~

    ~diff~ instruction, taking a use of instruction def, is now deprecated. We
    no longer follow the augmented graph paradigm, and instead adopt the
    /Wengert list/ (a.k.a. SSA form) source code transformation paradigm.

*** Add ~grad~
    We use ~grad~ instruction to implicitly call the gradient of a differentiable
    function. The transform pass ~autodiff~ will generate the automatically
    differentiated gradient function for every ~grad~ instruction, and replace
    every ~grad~ with ~call~.

*** Add ~elememt~
    We use ~element~ instruction, followed by a argument index and a use of
    tuple type, to get an argument of the tuple.

** Lift domain specific algorithms to TEL compiler

   We will remove domain specific algorithms, such as transformation passes in
   DLVM that emit cost functions and stochastic optimizers. These information
   should not be included in the same function as differentiable code. And such
   domain-specific and rather ad-hoc transformations should be done by the
   frontend compiler. Stochastic optimization and cost function can be defined
   in TEL with the simple syntax. 

* Benifits

  1. A standard IR, closer to LLVM.
  2. A uniform data flow graph--the def-use graph with multiple return values.
  3. Domain specific knowledge (NN training) is now handled by TEL. DLVM is now
     a "generic tensor code" compiler.

* Remaining Questions

  1. How do we, for example, only compute $\frac{\partial f(x, y)}{\partial x}$
     without computing $\frac{\partial f(x, y)}{\partial y}$? This was possible,
     though ad-hoc, using the section graph approach, but is not possible using
     the new approach (source code transformation with multiple returns) on the
     DLVM IR level. Perhaps a global optimization pass on HPVM/LLVM can nail
     this?
  2. How do we represent higher-order derivatives? For multivariable partial
     derivatives, every higher level incurs a magnitude of permutations. The
     higher-order differentiation of such function will need to deal with an
     \( \mathbb{R}^n \rightarrow \mathbb{R}^n \) function.

