#+TITLE: Deep Learning Virtual Machine
#+AUTHOR: Richard Wei

* Introduction
  We propose a programming model and a compiler infrastructure for deep learning
  architectures, a.k.a. various kinds of neural networks. Our compiler
  infrastructure, which we name Deep Learning Virtual Machine (DLVM), contains a
  high-level intermediate representation (IR) of data flow, a set of IR
  transformations such as the backpropagation algorithm and stochastic
  optimizations, and a back-end compiler based on HPVM and LLVM, a portable
  abstraction for heterogeneous parallel systems and a compiler infrastructure,
  respectively. DLVM, along with the capabilities of HPVM and LLVM, provide
  source- and target-independence, optimizations, and static code generation to
  neural network programs.

* TODO Previous Work

  Neural network toolkits are an essential part of deep learning applications.
  Most commonly, such toolkits are implemented as an interpreter of mathematical
  expressions composed of vector and matrix operations. Such operations are
  often executed in the form of parallel GPU compute kernels to achieve high
  performance. To use these implementations, users are expected to define a data
  flow graph using expressions in a host language, most commonly Python and Lua,
  specify training/inference configuration, and execute the the host program.
  The host program then dynamically interprets the data flow graph, check for
  tensor shape (a.k.a. dimensionality) errors, allocates memory on target
  devices, and interprets the expression by launching compute kernels per
  operation. 

** TODO TensorFlow

** TODO Torch
  
* Problems

** Targeting multiple architecture

   Currently, NVIDIA CUDA is the dominant architecture that toolkit developers
   adopt for deep neural networks. In these toolkits, there are two major kinds
   of implementations of parallelism: cuDNN/cuBLAS library calls and compiled
   CUDA kernels. Because compute kernels are often implemented per operator, per
   architecture, they are often not portable. Thus, each operator must be
   re-implemented for every other architecture. We believe that HPVM, a
   target-independent heterogeneous parallel instruction set architecture,
   solves exactly this problem.

# Save this for TEL
# ** Programming model
   
** TODO Kernel optimizations

   In current implementations, compute kernels are often implemented and
   launched per operator.

** Tuning for multiple devices

   In current implementations, execution across multiple heterogeneous parallel
   devices are almost completely hand-tuned. We strongly feel that this should
   be done by a good developer tool, not the developer.

* TODO Proposed Solution

** TODO High-level instruction set


** TODO Intermediate representation


** TODO Optimizations


** TODO Backpropagation and stochastic optimizations


** TODO Compilation
