#+TITLE: Deep Learning Virtual Machine
#+AUTHOR: Richard Wei

* Introduction
  We propose a programming model and a compiler infrastructure for deep learning
  architectures, a.k.a. various kinds of neural networks. Our compiler
  infrastructure, which we name Deep Learning Virtual Machine (DLVM), contains a
  high-level intermediate representation (IR) of data flow, a set of IR
  transformations such as the backpropagation algorithm and stochastic
  optimizations, and a back-end compiler based on HPVM and LLVM, a portable
  abstraction for heterogeneous parallel systems and a compiler infrastructure,
  respectively. DLVM, along with the capabilities of HPVM and LLVM, provide
  source- and target-independence, optimizations, and static code generation to
  neural network programs.

* TODO Previous Work

  Neural network toolkits are an essential part of deep learning applications.
  Most commonly, such toolkits are implemented as an interpreter of mathematical
  expressions composed of vector and matrix operations. Such operations are
  often executed in the form of parallel GPU compute kernels to achieve high
  performance. To use these implementations, users are expected to define a data
  flow graph using expressions in a host language, most commonly Python and Lua,
  specify training/inference configuration, and execute the the host program.
  The host program then dynamically interprets the data flow graph, check for
  tensor shape (a.k.a. dimensionality) errors, allocates memory on target
  devices, and interprets the expression by launching compute kernels per
  operation. 

  In these toolkits, the backpropagation algorithm is implemented using
  symbolic or more commonly reverse-mode automatic differentiation. In automatic
  differentiation, each operator knows how to differentiate itself. Along with
  remembered outputs from the forward pass, the backward pass automatically.

** TensorFlow

   
   
** Torch
  
* Problems

** Targeting multiple architectures

** Programming model
   
** Kernel optimizations

** Tuning for multiple devices

* 

