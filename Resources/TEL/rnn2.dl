module rnn2

declare input f32 [4x1] @i2
declare input f32 [4x1] @i1

declare parameter f32 [4x4] @W1 = random from f32 0.0 to f32 1.0
declare parameter f32 [8x8] @W2 = random from f32 0.0 to f32 1.0
declare parameter f32 [8x8] @W3 = random from f32 0.0 to f32 1.0
declare parameter f32 [4x1] @b1 = repeating f32 0.0
declare parameter f32 [8x1] @b2 = repeating f32 0.0
declare parameter f32 [8x1] @b3 = repeating f32 0.0

/// Output
declare output f32 [8x1] @o

/// Reference output for training
declare input f32 [8x1] @o.ref

/// Length of the sequence of reference output
declare input i32 @o.ref.t

/// Learning rate
declare input f32 @learning_rate

/// End of sentence symbol </s>
declare input f32 [8x1] @eos

declare buffer <f32 [8x1]> @x

/// Forward inference, generated by telc

!entry {
    %v0 = load f32 [4x1] @i1
    %v1 = mmul f32 [4x4] @W1, f32 [4x1] %v0
    %v2 = add f32 [4x1] %v1, f32 [4x1] @b1
    %h1 = sigmoid f32 [4x1] %v2
    %v3 = load f32 [4x1] @i2
    %v4 = concat f32 [4x1] %h1, f32 [4x1] %v3 along 0
    %v5 = sub f32 1.0, f32 [8x1] %v4
    %v6 = mmul f32 [8x8] @W2, f32 [8x1] %v5
    %v7 = add f32 [8x1] %v6, f32 [8x1] @b2
    %h2 = tanh f32 [8x1] %v7
    loop !decoder {
        %v8 = mmul f32 [8x8] @W3, f32 [8x1] %h2
        %v9 = add f32 [8x1] %v8, f32 [8x1] @b3
        %v10 = softmax f32 [8x1] %v9
        %o = argmax f32 [8x1] %v10
        store f32 [8x1] %o to f32 [8x1] @o
    } until f32 [8x1] @eos equals f32 [8x1] @o
}

/// Basic block extension for backpropagation, generated by GradientPass
/// This extension shall include:
/// 1. error evaluation (e.g. cross-entropy)
/// 2. the backprop algorithm
/// 3. stochastic optimizer (e.g. SGD)

extension !entry for backpropagation {
    loop extension !decoder for backpropagation {
        %v11 = load f32 [8x1] @o.ref
        ...
        store f32 [8x8] %W3.d to f32 [8x8] @W3
    } for @o.ref.t times
    ...
    store f32 [8x8] %W2.d to f32 [8x8] @W2
    ...
    store f32 [4x4] %W1.d to f32 [4x4] @W1
}
