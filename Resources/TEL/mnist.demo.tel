float net MNIST {
    W: [784x10]
    b: [1x10]

    inference(x: [?x784]) -> [?x10] {
        softmax(h * W + b)
    }

    loss(actual: [?x10], reference: [?x10]) -> [] {
        reduce(mean, (reference - actual)^2)
    }

    optimize<K*>(parameter: [K], gradient: [K],
                 iteration: [], learningRate: []) -> [K] {
        parameter - gradient .* learningRate
    }
}
