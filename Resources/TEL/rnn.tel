// A simple recurrent neural network described in TEL

// Universal data type for data flow
#type float16

// Parameter declarations
W1: param[4x4] = random(0.0, 1.0)
W2: param[4x2] = random(0.0, 1.0)
W3: param[2x2] = random(0.0, 1.0)
b1: param[4x1] = 0.0
b2: param[2x1] = 0.0
b3: param[2x1] = 0.0

// Recurrent block
recurrent t {
    // Input layer
    i: in[2x1]
    // Hidden layers
    h1: hidden[4x1] = sigmoid(W1 [i[t-1], h2[t-1]] + b1)
    h2: hidden[2x1] = tanh(W2 (1 - h1[t]) + b2)
}

// Output layer
o: out[2x1] = softmax(W3 h2 + b3)
